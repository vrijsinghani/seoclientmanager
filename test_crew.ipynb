{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing crew.kickoff() - Hello World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew, Agent, Task\n",
    "from crewai import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_callback_function(output):\n",
    "    print('Step Callback function called')\n",
    "    print(f\"step callback: {output}\")\n",
    "\n",
    "def task_callback_function(output):\n",
    "    print('Task_Callback function called')\n",
    "    print(f\"task callback: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE_URL = 'http://192.168.30.100:8000/'\n",
    "LITELLM_MASTER_KEY= 'sk-4048081138'\n",
    "OPENAI_API_BASE=\"http://192.168.30.100:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "import openai\n",
    "import requests\n",
    "import tiktoken\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class TokenCounterCallback(BaseCallbackHandler):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.llm = None\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        for prompt in prompts:\n",
    "            self.input_tokens += len(self.tokenizer.encode(prompt, disallowed_special=()))\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        for generation in response.generations:\n",
    "            for result in generation:\n",
    "                self.output_tokens += len(self.tokenizer.encode(result.text, disallowed_special=()))\n",
    "\n",
    "def get_models():\n",
    "    data=\"\"\n",
    "    #data = cache.get('models')\n",
    "    if not data:\n",
    "        url = f'{API_BASE_URL}/models'\n",
    "        headers = {'accept': 'application/json', 'Authorization': f'Bearer {LITELLM_MASTER_KEY}'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()['data']\n",
    "            # Sort the data by 'id' in ascending order\n",
    "            data.sort(key=lambda x: x['id'])\n",
    "        else:\n",
    "            return None\n",
    "    return [item['id'] for item in data]\n",
    "def get_llm(model_name:str, temperature=0.0):\n",
    "\n",
    "    tokenizer=tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_counter_callback = TokenCounterCallback(tokenizer)\n",
    "     \n",
    "    callback_manager = CallbackManager([token_counter_callback])  \n",
    "\n",
    "    llm = ChatOpenAI(model=model_name, \n",
    "                     base_url=API_BASE_URL, \n",
    "                     api_key=LITELLM_MASTER_KEY, \n",
    "                     temperature=temperature, \n",
    "                     callbacks=callback_manager)\n",
    "\n",
    "    token_counter_callback.llm = llm\n",
    "    return llm, token_counter_callback\n",
    "\n",
    "def get_crew_llm(model_name:str, temperature=0.0):\n",
    "\n",
    "\n",
    "    llm = ChatOpenAI(model=model_name, \n",
    "                     base_url=API_BASE_URL, \n",
    "                     api_key=LITELLM_MASTER_KEY, \n",
    "                     temperature=temperature, \n",
    "    )\n",
    "\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm: callbacks=<langchain_core.callbacks.manager.CallbackManager object at 0x7916c8198ca0> client=<openai.resources.chat.completions.Completions object at 0x7916c819aa70> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7916c838bf10> root_client=<openai.OpenAI object at 0x7916c8199900> root_async_client=<openai.AsyncOpenAI object at 0x7916c819afe0> model_name='ollama/llama3.2:3b-instruct-q8_0' temperature=0.0 openai_api_key=SecretStr('**********') openai_api_base='http://192.168.30.100:8000/' openai_proxy=''\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m=\"claude-3-haiku-20240307\"\n",
    "m=\"llama3.2:3b-instruct-q8_0\"\n",
    "m=\"ollama/deepseek-coder-v2:16b-lite-instruct-q8_0\"\n",
    "m=\"ollama/llama3.2:3b-instruct-q8_0\"\n",
    "\n",
    "llm,_ = get_llm(m)\n",
    "print(f\"llm: {llm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple agent\n",
    "agent = Agent(\n",
    "    role='Greeter',\n",
    "    goal='Greet the world with enthusiasm',\n",
    "    backstory='You are an AI assistant designed to greet people warmly.',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    #llm=llm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poet = Agent(\n",
    "    role='poet',\n",
    "    goal='Write a Haiku',\n",
    "    backstory='You are an AI assistant designed to write haikus.',\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    ")\n",
    "# Create a simple task\n",
    "task = Task(\n",
    "    description='Create a warm greeting for the world',\n",
    "    expected_output='Hello, world!',\n",
    "    agent=agent\n",
    ")\n",
    "\n",
    "poem = Task(\n",
    "    description=\"write a haiku\",\n",
    "    expected_output=\" a haiku poem\",\n",
    "    agent=poet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 10:32:10,750 - 133141809354560 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGreeter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCreate a warm greeting for the world\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mGreeter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Hello, world! Welcome to our vibrant and diverse planet. I'm Greeter, your friendly AI assistant, here to spread warmth and enthusiasm wherever you go. May today bring you joy, love, and all the things that make life beautiful.\u001b[00m\n",
      "\n",
      "\n",
      "Task_Callback function called\n",
      "task callback: Hello, world! Welcome to our vibrant and diverse planet. I'm Greeter, your friendly AI assistant, here to spread warmth and enthusiasm wherever you go. May today bring you joy, love, and all the things that make life beautiful.\n",
      "Crew Result:\n",
      "Hello, world! Welcome to our vibrant and diverse planet. I'm Greeter, your friendly AI assistant, here to spread warmth and enthusiasm wherever you go. May today bring you joy, love, and all the things that make life beautiful.\n"
     ]
    }
   ],
   "source": [
    "# Create the crew\n",
    "crew = Crew(\n",
    "    agents=[agent],\n",
    "    tasks=[task],\n",
    "    verbose=True,\n",
    "    task_callback=task_callback_function,\n",
    "    \n",
    ")\n",
    "# Run the crew\n",
    "result = crew.kickoff()\n",
    "\n",
    "print(\"Crew Result:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
