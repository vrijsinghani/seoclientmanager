{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.v1 import BaseModel, Field, PrivateAttr\n",
    "from crewai_tools import BaseTool\n",
    "from typing import Optional, Type, Any\n",
    "import logging\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from browser_tool import BrowserTool  # Make sure this import works\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CrawlWebsiteToolSchema(BaseModel):\n",
    "    \"\"\"Input for CrawlWebsiteTool.\"\"\"\n",
    "    website_url: str = Field(..., description=\"Mandatory website url to crawl and read content\")\n",
    "\n",
    "class FixedCrawlWebsiteToolSchema(BaseModel):\n",
    "    \"\"\"Input for CrawlWebsiteTool when website_url is fixed.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class CrawlWebsiteTool(BaseTool):\n",
    "    name: str = \"Crawl and read website content\"\n",
    "    description: str = \"A tool that can be used to crawl a website and read its content, including content from internal links on the same page.\"\n",
    "    args_schema: Type[BaseModel] = CrawlWebsiteToolSchema\n",
    "\n",
    "    website_url: Optional[str] = None\n",
    "    max_pages: int = 10\n",
    "    _browser_tool: BrowserTool = PrivateAttr()\n",
    "\n",
    "    def __init__(self, website_url: Optional[str] = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        logger.info(\"Initializing CrawlWebsiteTool\")\n",
    "        if website_url is not None:\n",
    "            self.website_url = website_url\n",
    "\n",
    "    def _run(self, website_url: str) -> str:\n",
    "        logger.info(f\"Processing {website_url}\")\n",
    "        content = self._crawl_website(website_url)\n",
    "        return content\n",
    "\n",
    "    def _crawl_website(self, url: str) -> str:\n",
    "        content = \"\"\n",
    "        visited_urls = set()\n",
    "        urls_to_visit = deque([url])\n",
    "        base_domain = urlparse(url).netloc\n",
    "\n",
    "        while urls_to_visit and len(visited_urls) < self.max_pages:\n",
    "            current_url = urls_to_visit.popleft()\n",
    "            if current_url in visited_urls:\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"Visiting: {current_url}\")\n",
    "            try:\n",
    "                page_content = self._browser_tool.get_content(current_url)\n",
    "                content += f\"---link: {current_url}\\n{page_content}\\n---page-end---\\n\"\n",
    "                visited_urls.add(current_url)\n",
    "\n",
    "                links = self._browser_tool.get_links(current_url)\n",
    "                for link in links:\n",
    "                    if link not in visited_urls and urlparse(link).netloc == base_domain:\n",
    "                        urls_to_visit.append(link)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {current_url}: {e}\")\n",
    "                content += f\"---link: {current_url}\\nError: Failed to process this page\\n---page-end---\\n\"\n",
    "\n",
    "        return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 13:50:43,793 - 126323511457600 - 3650673014.py-3650673014:65 - ERROR: Error processing https://brandon.neuralami.com: 'ModelPrivateAttr' object has no attribute 'get_content'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---link: https://brandon.neuralami.com\n",
      "Error: Failed to process this page\n",
      "---page-end---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this code block to test the CrawlWebsiteTool on the specified URL\n",
    "\n",
    "# Create an instance of the CrawlWebsiteTool\n",
    "crawler_tool = CrawlWebsiteTool()\n",
    "\n",
    "# Run the crawler on the specified website\n",
    "extracted_content = crawler_tool._run(\"https://brandon.neuralami.com\")\n",
    "\n",
    "# Print the extracted content\n",
    "print(extracted_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seomanager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
