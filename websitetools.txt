This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-30T18:38:33.019Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
browser_tool/
  browser_tool.py
  test_browser_tool.py
crawl_website_tool/
  crawl_website_tool.py
  test_crawl_website_tool.py

================================================================
Repository Files
================================================================

================
File: browser_tool/browser_tool.py
================
import json
import os
from typing import Any, Type, Set
import logging
import re
from tenacity import retry, stop_after_attempt, wait_exponential

import requests
from pydantic import BaseModel, Field
import html2text
from crewai_tools import BaseTool
from urllib.parse import urljoin, urlparse
import dotenv
from bs4 import BeautifulSoup
import trafilatura
from readability.readability import Document

dotenv.load_dotenv()

logger = logging.getLogger(__name__)

class BrowserToolSchema(BaseModel):
  """Input for BrowserTool."""
  website: str = Field(..., title="Website", description="Full URL of the website to scrape (e.g., https://google.com)")
  output_type: str = Field(
      default="text",
      title="Output Type",
      description="Type of output desired: 'text' for cleaned content or 'raw' for HTML"
  )

class BrowserTool(BaseTool):
  name: str = "Scrape website content"
  description: str = "A tool that can be used to scrape website content. Pass a string with only the full URL, no need for a final slash `/`. output_type can be 'text' for cleaned content or 'raw' for HTML."
  output_type: str = "text"
  tags: Set[str] = {"browser", "scrape", "website", "content"}
  args_schema: Type[BaseModel] = BrowserToolSchema
  api_key: str = Field(default=os.environ.get('BROWSERLESS_API_KEY'))
  base_url: str = Field(default="https://browserless.rijsinghani.us/scrape")

  def __init__(self, **data):
      super().__init__(**data)
      if not self.api_key:
          logger.error("BROWSERLESS_API_KEY is not set in the environment variables.")

  def _run(
      self,
      website: str,
      output_type: str = "text",
      **kwargs: Any,
  ) -> Any:
      """Scrape website content."""
      website = self.normalize_url(website)
      logger.info(f"Scraping website: {website} with output type: {output_type}")
      content = self.get_content(website, output_type)
      return f'\nContent of {website}: {content}\n'

  def normalize_url(self, url: str) -> str:
      """Normalize the URL by adding the protocol if missing."""
      if not re.match(r'^\w+://', url):
          return f"https://{url}"
      return url

  def clean_content(self, content: str) -> str:
      """Clean and structure extracted content"""
      if not content:
          return ""
          
      # Remove excessive whitespace
      content = re.sub(r'\s+', ' ', content)
      
      # Remove common boilerplate phrases
      boilerplate = [
          'cookie policy',
          'accept cookies',
          'privacy policy',
          'terms of service',
          'subscribe to our newsletter',
          'sign up for our newsletter',
          'all rights reserved',
      ]
      for phrase in boilerplate:
          content = re.sub(rf'(?i){phrase}.*?[\.\n]', '', content)
      
      # Remove email addresses and phone numbers
    #   content = re.sub(r'\S+@\S+', '[EMAIL]', content)
    #   content = re.sub(r'\+?\d{1,3}[-.\s]?$?\d{3}$?[-.\s]?\d{3}[-.\s]?\d{4}', '[PHONE]', content)
      
      return content.strip()

  def detect_content_type(self, url: str, html_content: str) -> str:
      """Detect type of content for specialized handling"""
      patterns = {
          'article': r'article|post|blog',
          'product': r'product|item|price',
          'documentation': r'docs|documentation|api|reference',
      }
      
      for content_type, pattern in patterns.items():
          if re.search(pattern, url, re.I) or re.search(pattern, html_content, re.I):
              return content_type
      return 'general'

  def extract_content(self, url: str, html_content: str) -> dict:
      """Multi-strategy content extraction"""
      content = {
          'title': '',
          'text': '',
          'metadata': {}
      }
      
      try:
          # Strategy 1: Trafilatura
          trafilatura_content = trafilatura.extract(html_content, include_comments=False, 
                                                  include_tables=True, 
                                                  include_links=False)
          
          # Strategy 2: Readability
          doc = Document(html_content)
          readable_article = doc.summary()
          doc_title = doc.title()
          
          # Strategy 3: BeautifulSoup fallback
          soup = BeautifulSoup(html_content, 'lxml')
          
          # Combine results with priority
          if trafilatura_content:
              content['text'] = trafilatura_content
          elif readable_article:
              soup_readable = BeautifulSoup(readable_article, 'lxml')
              content['text'] = soup_readable.get_text(separator=' ', strip=True)
          else:
              content['text'] = soup.get_text(separator=' ', strip=True)
              
          # Extract title
          content['title'] = doc_title or soup.title.string if soup.title else ''
          
          # Extract basic metadata
          meta_tags = soup.find_all('meta')
          content['metadata'] = {
              'description': next((tag.get('content', '') for tag in meta_tags if tag.get('name', '').lower() == 'description'), ''),
              'keywords': next((tag.get('content', '').split(',') for tag in meta_tags if tag.get('name', '').lower() == 'keywords'), []),
              'author': next((tag.get('content', '') for tag in meta_tags if tag.get('name', '').lower() == 'author'), '')
          }
          
      except Exception as e:
          logger.error(f"Error in content extraction: {str(e)}")
          # Fallback to basic HTML extraction
          content['text'] = html2text.html2text(html_content)
              
      return content

#  @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
  def get_content(self, url: str, output_type: str = "text") -> str:
      """Scrape website content with retry mechanism."""
      payload = {
          "url": url,
          "elements": [{"selector": "body"}]
      }
      headers = {'cache-control': 'no-cache', 'content-type': 'application/json'}
      
      try:
          response = requests.post(f"{self.base_url}?token={self.api_key}", 
                                headers=headers, 
                                json=payload,
                                timeout=30)
          response.raise_for_status()
          data = response.json()
          html_content = data['data'][0]['results'][0]['html']

          # Return raw HTML if requested
          if output_type.lower() == "raw":
              return html_content

          # Otherwise process the content as before
          content_type = self.detect_content_type(url, html_content)
          extracted_content = self.extract_content(url, html_content)
          
          # Format the final output
          result = f"Title: {extracted_content['title']}\n\n"
          result += f"Content Type: {content_type}\n\n"
          result += extracted_content['text']

          # Add metadata if available
          if extracted_content['metadata'].get('description'):
              result += f"\n\nDescription: {extracted_content['metadata']['description']}"
          if extracted_content['metadata'].get('author'):
              result += f"\nAuthor: {extracted_content['metadata']['author']}"
          if extracted_content['metadata'].get('keywords'):
              result += f"\nKeywords: {', '.join(extracted_content['metadata']['keywords'])}"
          return result
          
      except requests.exceptions.RequestException as e:
          logger.error(f"Failed to fetch content for {url}: {str(e)}")
          logger.error(f"Response status code: {getattr(response, 'status_code', 'N/A')}")
          logger.error(f"Response content: {getattr(response, 'text', 'N/A')}")
          return ""

  def get_links(self, url: str) -> Set[str]:
      """Extract links from the webpage."""
      payload = {
          "url": url,
          "elements": [
              {"selector": "a"}
          ]
      }
      headers = {'Cache-Control': 'no-cache', 'Content-Type': 'application/json'}
      try:
          response = requests.post(f"{self.base_url}?token={self.api_key}", 
                                headers=headers, 
                                json=payload,
                                timeout=30)
          response.raise_for_status()
          data = response.json()
          links = set()
          base_domain = urlparse(url).netloc
          for element in data['data'][0]['results']:
              for attr in element['attributes']:
                  if attr['name'] == 'href':
                      full_url = urljoin(url, attr['value'])
                      if urlparse(full_url).netloc == base_domain:
                          links.add(full_url)
          return links
      except requests.exceptions.RequestException as e:
          logger.error(f"Failed to fetch links for {url}: {str(e)}")
          logger.error(f"Response status code: {getattr(response, 'status_code', 'N/A')}")
          logger.error(f"Response content: {getattr(response, 'text', 'N/A')}")
          return set()

================
File: browser_tool/test_browser_tool.py
================
import logging
import sys
import os

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
sys.path.insert(0, project_root)

from apps.agents.tools.browser_tool.browser_tool import BrowserTool

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_browser_tool():
    # Initialize the tool
    browser = BrowserTool()

    # Test get_content
    test_url = "https://brandon.neuralami.com"
    logger.info(f"Testing get_content for {test_url}")
    content = browser.get_content(test_url)
    logger.info(f"Content length: {len(content)}")
    logger.info(f"Content preview: {content[:200]}...")
    print(content[:200])

    # Test get_links
    logger.info(f"Testing get_links for {test_url}")
    links = browser.get_links(test_url)
    logger.info(f"Number of links found: {len(links)}")
    logger.info(f"Links: {links}")
    print(links)
if __name__ == "__main__":
    test_browser_tool()

================
File: crawl_website_tool/crawl_website_tool.py
================
import logging
from typing import Optional, Dict, Any, Type
from pydantic import BaseModel, Field
from crewai_tools.tools.base_tool import BaseTool
from urllib.parse import urljoin, urlparse
from spider_rs import Website, Page
from trafilatura import extract


logger = logging.getLogger(__name__)

class CrawlWebsiteToolSchema(BaseModel):
    website_url: str = Field(description="Mandatory website url to crawl and read content")

    model_config = {
        "extra": "forbid"
    }      

class CrawlWebsiteTool(BaseTool):
    name: str = "Crawl and read website content"
    description: str = "A tool that can be used to crawl a website and read its content, including content from internal links on the same page."
    args_schema: Type[CrawlWebsiteToolSchema] = CrawlWebsiteToolSchema
    website_url: Optional[str] = None
    
    def __init__(self, website_url: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        if website_url is not None:
            self.website_url = website_url
            self.description = f"A tool that can be used to crawl {website_url} and read its content, including content from internal links on the same page."

    def _run(self, website_url: str) -> str:
        url = website_url or self.website_url
        if not url:
            raise ValueError("No website URL provided")

        logger.info(f"Starting crawl for URL: {url}")
        
        try:
            result = self._crawl_website(url)
            return result
        except Exception as e:
            logger.error(f"Error during crawl: {e}")
            raise

    def _crawl_website(self, start_url: str) -> str:  # Changed return type to str
        website = Website(start_url)
        website.with_budget({"*": 50})  # Set a limit for crawling
        website.with_respect_robots_txt(True)
        website.with_subdomains(False)  # Stick to the main domain
        website.with_tld(False)  # Don't crawl top-level domain
        website.with_delay(1)  # Be respectful with a 1-second delay between requests
        content = ""

        website.scrape()
        
        pages = website.get_pages()
        
        for page in pages:
            page_content = self._extract_content(page)
            content += page_content
        
        return content  # Return the content directly

    def _extract_content(self, page: Page) -> str:
        try:
            html_content = page.content
            extracted_content = extract(html_content)
            logger.info(f"Extracted content length for {page.url}: {len(extracted_content) if extracted_content else 0}")
            return f"---link: {page.url}\n{extracted_content}\n---page-end---\n" if extracted_content else ""
        except Exception as e:
            logger.error(f"Error extracting content from {page.url}: {e}")
            return ""

================
File: crawl_website_tool/test_crawl_website_tool.py
================
import logging
import sys
import os

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
sys.path.insert(0, project_root)

from apps.agents.tools.crawl_website_tool.crawl_website_tool import CrawlWebsiteTool

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_crawl_website():
    # Initialize the tool
    crawler = CrawlWebsiteTool()

    # Test with a real website
    test_url = "https://proplankohio.com"
    logger.info(f"Starting crawl of {test_url}")
    print(f"Starting crawl of {test_url}")
    result = crawler._run(test_url)

    # Print the result
    logger.info(f"Crawl result for {test_url}:")
    logger.info(result)
    print(result)

    # You can add more assertions here to verify the output

if __name__ == "__main__":
    test_crawl_website()
